{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "SageMaker Project.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1joEXT8dOxv0",
        "outputId": "5d8b12ec-941b-4213-ed44-8f1c0841db17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%mkdir ../data\n",
        "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-04 14:36:19--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  18.7MB/s    in 5.9s    \n",
            "\n",
            "2021-09-04 14:36:25 (13.7 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxKUJQCJOxv1"
      },
      "source": [
        "## Preparing and Processing the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJMqwkVZOxv1"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='../data/aclImdb', test_to_train_num=10000):\n",
        "    data = {}\n",
        "    labels = {}\n",
        "    \n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "        \n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "            \n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "            \n",
        "            for f in files:\n",
        "                with open(f) as review:         \n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
        "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
        "    \n",
        "    if test_to_train_num:    \n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data['train'][sentiment].extend(data['test'][sentiment][:test_to_train_num])\n",
        "            labels['train'][sentiment].extend(labels['test'][sentiment][:test_to_train_num])\n",
        "            data['test'][sentiment] = data['test'][sentiment][test_to_train_num:]\n",
        "            labels['test'][sentiment] = labels['test'][sentiment][test_to_train_num:]     \n",
        "    return data, labels"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T0pGMBnOxv1",
        "outputId": "57882efb-03c6-4ae0-d470-7003e4783258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# distribute more reviews for training\n",
        "data, labels = read_imdb_data(test_to_train_num=10000)\n",
        "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "            len(data['train']['pos']), len(data['train']['neg']),\n",
        "            len(data['test']['pos']), len(data['test']['neg'])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDB reviews: train = 22500 pos / 22500 neg, test = 2500 pos / 2500 neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJDfylUZOxv2"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data, labels):\n",
        "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
        "    \n",
        "    #Combine positive and negative reviews and labels\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "    \n",
        "    #Shuffle reviews and corresponding labels within training and test sets\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "    \n",
        "    # Return a unified training data, test data, training labels, test labets\n",
        "    return data_train, data_test, labels_train, labels_test"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2GI5_t7Oxv3",
        "outputId": "2038eb6a-758e-4b21-8f33-fb5ed46bfb35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
        "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDb reviews (combined): train = 45000, test = 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tseSeV3_Oxv4"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def review_to_words(review):\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "    stemmer = PorterStemmer()\n",
        "    \n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "    \n",
        "    return words"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yihQAGOLOxv5"
      },
      "source": [
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "        \n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STiMM_JoOxv5",
        "outputId": "8c9f0b33-31db-4343-c2e1-c2994e8dd1d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Preprocess data\n",
        "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuvh2On2Oxv6"
      },
      "source": [
        "### Create a word dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHziiJ9jOxv6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_dict(data, vocab_size = 5000):\n",
        "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
        "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur  \n",
        "    counter = Counter()\n",
        "    \n",
        "    for sentence in data:\n",
        "        counter.update(sentence)\n",
        "    \n",
        "    sorted_words = sorted(counter.keys(), reverse=True, key=lambda x: counter[x])\n",
        "    \n",
        "    word_dict = {}\n",
        "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
        "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
        "        \n",
        "    return word_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXXAOK26Oxv6"
      },
      "source": [
        "word_dict = build_dict(train_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Eg0ykHOOxv7"
      },
      "source": [
        "data_dir = '../data/pytorch' # The folder we will use for storing data\n",
        "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
        "    os.makedirs(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES7_sZLBOxv7"
      },
      "source": [
        "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
        "    pickle.dump(word_dict, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8yFpqAyOxv8"
      },
      "source": [
        "### Transform and pad the reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddlww5UnOxv8"
      },
      "source": [
        "def convert_and_pad(word_dict, sentence, pad=500):\n",
        "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
        "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
        "    \n",
        "    working_sentence = [NOWORD] * pad\n",
        "    \n",
        "    for word_index, word in enumerate(sentence[:pad]):\n",
        "        if word in word_dict:\n",
        "            working_sentence[word_index] = word_dict[word]\n",
        "        else:\n",
        "            working_sentence[word_index] = INFREQ\n",
        "            \n",
        "    return working_sentence, min(len(sentence), pad)\n",
        "\n",
        "def convert_and_pad_data(word_dict, data, pad=500):\n",
        "    result = []\n",
        "    lengths = []\n",
        "    \n",
        "    for sentence in data:\n",
        "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
        "        result.append(converted)\n",
        "        lengths.append(leng)\n",
        "        \n",
        "    return np.array(result), np.array(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukeiKXrbOxv8"
      },
      "source": [
        "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
        "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKrp-l8bOxv-"
      },
      "source": [
        "## Step 4: Build and Train the PyTorch Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjppqyECXSpQ",
        "outputId": "161008c3-cad5-480f-d5c6-42258df455e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile MLproject\n",
        "name: tutorial\n",
        "\n",
        "conda_env: conda.yaml\n",
        "\n",
        "entry_points:\n",
        "  main:\n",
        "    parameters:\n",
        "      alpha: float\n",
        "      l1_ratio: {type: float, default: 0.1}\n",
        "    command: \"python train.py {vocab_size} {embedding_dim} {hidden_dim}\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MLproject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXc88WJaXTb5",
        "outputId": "6514e222-d3a1-4ddb-9c5f-05cb2d01a20a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile conda.yaml\n",
        "name: tutorial\n",
        "channels:\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - numpy>=1.14.3\n",
        "  - pandas>=1.0.0\n",
        "  - scikit-learn=0.19.1\n",
        "  - pip\n",
        "  - pip:\n",
        "    - mlflow"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing conda.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2CPeGpFX64q"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "MLFLOW_SERVER_URL = 'http://127.0.0.1:5000/'\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN4i4FO5YO9s"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH6gLWMkYReV",
        "outputId": "9697ea98-6af8-4365-d405-8aaf827d979f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_bTNBgDYYEx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}